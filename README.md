# coqa-bert-baselines
BERT baselines for extractive question answering on coqa (https://stanfordnlp.github.io/coqa/)

## TODO

- [x] [BERT](https://arxiv.org/pdf/1810.04805.pdf)
- [ ] [RoBERTa](https://arxiv.org/abs/1907.11692)
- [x] [DistilBERT](https://github.com/huggingface/transformers/tree/master/examples/distillation)
- [ ] [SpanBERT](https://arxiv.org/abs/1907.10529)

## Results
|Model Name| Dev F1 | Dev EM |
|----------|--------|--------|
| BERT | 63.08 | 53.03 |
| DistilBERT | 61.5 | 52.35 |

